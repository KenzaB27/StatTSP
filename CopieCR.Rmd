---
title: "TP Statistique"
author: "BOUZID Kenza, JEANNE Nathan, CANNEDDU Hugo"
date: "1 avril 2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
set.seed(287)
knitr::opts_chunk$set(echo = TRUE)

## il est possible qu'avant d'installer le package TSPpackage vous deviez installer ou ré-installer Rcpp
#install.packages('Rcpp')

# install.packages('./TSPpackage_1.0.tar.gz',repos=NULL,type='bin') ## pour linux
# install.packages('./TSPpackage_1.0.zip',repos=NULL,type='bin')    ## pour windows
## je ne peux pas fournir de package pour mac...

## Appels aux packages, après les avoir installés !
library(sp)
library(maps)
library(microbenchmark)
library(TSP)
library(TSPpackage)
```
Voici le plan de ce qui sera fait dans le TP.

# 0. Visualisation de chemins

Lecture du fichier des villes :

```{r, echo=TRUE}
villes <- read.csv('DonneesGPSvilles.csv',header=TRUE,dec='.',sep=';',quote="\"")
str(villes)
```
Représentation des chemins par plus proches voisins et du chemin optimal :
```{r, echo=TRUE}
coord <- cbind(villes$longitude,villes$latitude)
dist <- distanceGPS(coord)
voisins <- TSPnearest(dist)

pathOpt <- c(1,8,9,4,21,13,7,10,3,17,16,20,6,19,15,18,11,5,22,14,12,2)

par(mfrow=c(1,2),mar=c(1,1,2,1))
plotTrace(coord[voisins$chemin,], title='Plus proches voisins')
plotTrace(coord[pathOpt,], title='Chemin optimal')
```


Les longueurs des trajets (à vol d'oiseau) valent respectivement, pour la méthode des plus proches voisins :
```{r, echo=FALSE}
voisins$longueur
```
et pour la méthode optimale :
```{r, echo=FALSE}
calculeLongueur(dist,pathOpt)
```

Ceci illustre bien l'intérêt d'un algorithme de voyageur de commerce. Nous allons dans la suite étudier les performances de cet algorithme.


# 1. Comparaison d'algorithmes

Nombre de sommets fixes et graphes "identiques".

```{r, echo=TRUE}
      n <- 10
sommets <- data.frame(x = runif(n), y = runif(n))
  couts <- distance(sommets)
```

## 1.1. Longueur des chemins

Comparaison des longueurs de différentes méthodes : 

   * boxplots
   
```{r, echo=FALSE}
  X1 <- vector(mode="integer", length=50)
  X2 <- vector(mode="integer", length=50)
  X3 <- vector(mode="integer", length=50)
  X4 <- vector(mode="integer", length=50)
  X5 <- vector(mode="integer", length=50)
  n <-10
  for(i in 1:50)
  {
    sommets <- data.frame(x = runif(n), y = runif(n))
    couts <- distance(sommets)
    v1<-TSPbranch(couts)
    v2<-TSPnearest(couts)
    v3<-TSPsolve(couts, "repetitive_nn")
    v4<-TSPsolve(couts, "farthest_insertion")
    v5<-TSPsolve(couts, "two_opt")
    X1[i]<-v1
    X2[i]<-v2$longueur
    X3[i]<-v3
    X4[i]<-v4
    X5[i]<-v5
  }
  
  U1 <- rnorm(100)
  U2 <- rnorm(100,5)
  U3 <- rnorm(100,0,8)
  U4 <- rnorm(100,10)
  U5 <- rnorm(100,10,3)
  
  
  mat <- cbind(X1,X2,X3,X4,X5)
  colnames(mat)<-c("Branch","Nearest","repetitive_nn","insertion","two_opt")
  par(mfrow=c(1,1))
  boxplot(mat,notch=TRUE)
```   
#TODO : Commenter le diagramme a moustaches (hihi)


   * test entre 'nearest' et 'branch'
   
   

   * tests 2 à 2 
```{r, echo=FALSE}
results<- c(X1,X2,X3,X4,X5)
methods<- c(rep("branch", 50),rep("nearest", 50),rep("repetitive_nn", 50),rep("farthest_insertion", 50),rep("two_opt", 50))

pairwise.t.test(results,methods,adjust.method="bonferroni")
```
#TODO : conclure 
# Note : Si la p-value est 1, 100% de chance que ????

## 1.2. Temps de calcul

Comparaison des temps à l'aide du package microbenchmark.

Exemple d'application de microbenchmark :
```{r, echo=TRUE}
microbenchmark(TSPbranch(couts),TSPnearest(couts), TSPsolve(couts, "repetitive_nn"), TSPsolve(couts, "farthest_insertion"), TSPsolve(couts, "two_opt"), times=20, setup={ n<-10
  sommets <- data.frame(x = runif(n), y = runif(n))
  couts <- distance(sommets)
})
```

# 2. Etude e la complexité de l'algorithme Branch and Bound

## 2.1. Comportement par rapport au nombre de sommets : premier modèle

Récupération du temps sur 10 graphes pour différentes valeurs de $n$.

````{r, echo=TRUE}
seqn <- seq(4,20,1)

#calcul de temps
temps<-matrix(0,nrow=17,ncol=10)
for ( i in 1:length(seqn)){
  temps[i,]<-microbenchmark(TSPsolve(couts, method = "branch"),
                           times = 10,
                           setup = { n <- seqn[i]
                           couts <- distance(cbind(x = runif(n), y = runif(n)))}
  )$time
}
````
Visualisation de temps en fonction de n puis de $\log(temps)^2$ en fonction de n: 

```` {r, echo=FALSE}
# representation de temps

par(mfrow=c(1,2)) # 2 graphiques sur 1 ligne
matplot(seqn, temps, type = 'p', xlab='n', ylab='temps')
matplot(seqn, log(temps)^2, type ='p', xlab='n', ylab=expression(log(temps)^2))
````

Nous remarquons le comportement exponentionnelle du temps en fonction de n, ainsi il s'agit bien d'une régression linéaire pour $\log(temps)^2$  en fonction de n. 

Ajustement du modèle linéaire de $\log(temps)^2$ en fonction de $n$.

```` {r, echo = FALSE}
# Ajuter le modèle linéaire de log(temps)^2

vect_temps <- log(as.vector(temps))^2
vect_dim <- rep(seqn,times=10)
temps.lm <- lm(vect_temps ∼ vect_dim)
summary(temps.lm)

````

####Analyse de la validité du modèle : 

A partir des résultats obtenus, nous remarquons un ration $R^2$ de `r summary(temps.lm)$r.squared` qui est assez proche de 1. Cela signifie que les observations s'éloignent un peu du modèle prédit. En effet, le graphique observé montre une courbe pas tout à fait linéaire. 

Afin de visualiser l'écart au modèle, nous traçons la courbe suivante: 
```{r,echo=FALSE}
matplot(seqn, log(temps)^2, xlab='n', ylab=expression(log(temps)^2))
abline(temps.lm)
```

Nous remarquons que les données suivent bien un modèle linéaire, cependant l'écart observé reste important. 

####pertinence des coefficients et du modèle:
  
A partir des informations fournies par R autour du modèle étudié, nous remarquons: 
    + des coefficients avec un taux d'erreur (écart-type) important allant jusqu'à 5.3 pour l'ordonnée à l'origine et 0.4 pour le cofficient directeur, nous ne pouvons donc pas affirmer la pertinence des coefficients du modèle linéaire.
    + la p-valeur de chacun des coefficients est très petite inférieure à $2.2\exp(-16)$

Ainsi, nous rejetons l'hypothèse que le $\log(temps)^2$ d'exécution de l'algorithme Branch and Bound suit un modèle linéaire.
  
####étude des hypothèses sur les résidus.
  Les hypothèses sur les résidus à étudier sont les suivantes: 
    + Loi normale
    + Espérance nulle 
    + Variance constante 
    + indépendance
  Les graphiques suivants nous permettent de valider oupas ces dernières. 
```{r, echo=FALSE}
par(mfrow=c(2,2)) # 4 graphiques, sur 2 lignes et 2 colonnes
plot(temps.lm)
```

  Compte tenue des résultats obtenus, nous remarquons: 
    + Les graphiques Residuals vs Fitted et Scale-Location représentent des nuages de points très écartés ainsi la variance des résidus n'est pas constante. Ainsi on dit que les résidus sont hétéroscédastiques. Les points ne présentent une tendance trop marquée sur le graphique, nous pouvons ainsi sire que l'espérance est nulle.
    + Le graphique Normal Q-Q semble être presque complétement aligné avec un éloignement de la droite au niveau des extrémités, nous aurons donc envie d'affirmer que l'hypothèse de Loi normale est vraie (à confirmer avec d'autres tests cf test de Shapiro-Wilk)
    + Le graphique Residuals vs Leverage montre l'influence des échantillons. Nous remarquons l'existence d'outliers (points très eloignés des autres). Ainsi, une multitude de points ne contribuent pas à la construction de la droite du modèle linéaire. 

Au vu du rejet de quelques unes des 4 hypothèses, le modèle n'est plus valable. Vérifions si les résidus suivent quand même une loi normale.

Pour vérifier si les résidus suivent une loi normale, nous faisons le test de Shapiro-Wilk : 
  
```{r, echo=FALSE}
shapiro.test(residuals(temps.lm))
```
Nous obtenons une p-valeur très petite <5% ainsi les résidus suivent bien une loi normale. 

## 2.2. Comportement par rapport au nombre de sommets : étude du comportement moyen

Récupération du temps moyen et traçage des courbes: 

```{r}
temps.moy <- rowMeans(temps)
vect_temps_moy <- log(as.vector(temps.moy))^2
vect_dim <- seqn
temps.moy_lm <- lm(vect_temps_moy~vect_dim)
par(mfrow=c(1,2)) # 2 graphiques sur 1 ligne
matplot(vect_dim, temps.moy, xlab='n', ylab='temps')
matplot(vect_dim, vect_temps_moy, xlab='dimension', ylab=expression(log(temps_moy)^2))
```

La courbe *temps_moyen* en fonction de *n* suit une tendance exponentielle. Ainsi comme la 1ère partie, la courbe $\log(temps.moy)^2$ en fonction de $n$ devrait correspondre à un modèle linéaire comme le montre le graphique.

### Ajustement du modèle linéaire de $\log(temps.moy)^2$ en fonction de $n$.

```` {r, echo = FALSE}
# Ajuter le modèle linéaire de log(temps)^2
summary(temps.moy_lm)
````

#### Analyse de la validité du modèle : 

A partir du résumé réalisé par R concernant le modèle linéaire, nous remarquons encore une fois des coefficients pas du tout correctes (car écart très important pour les deux coefficients Intercept et vect_dim allant juqu'à 11,93 pour l'ordonnée à l'origine). De plus les p-valeurs de ces derniers sont très très faibles. Le coeeficient R-squared est égal à 0.936 et se rapproche fortement de 1. 
Nous rejettons ainsi l'hypothèse d'un modèle linéaire.

##### Etude des hypothèses sur les résidus.
  
```{r}
par(mfrow=c(2,2)) # 4 graphiques, sur 2 lignes et 2 colonnes
plot(temps.moy_lm)
```

    + Les graphiques Residuals vs Fitted et Scale-Location représentent des points peu répartis selon les abscisses ainsi la variance des résidus est constante. Les points présentent une tendance trop marquée sur le graphique, ainsi l'espérance n'est pas nulle.
    + Le graphique Normal Q-Q semble n'est pas du tout aligné donc les résidus ne suivent pas une loi normale.
    + Le graphique Residuals vs Leverage montre l'influence des échantillons. Nous ne remarquons pas l'existence d'outliers (points très eloignés des autres ou spécialement en dehors des bornes par rapport à la distance de Cook). Ainsi, une multitude de points contribuent à la construction de la droite du modèle linéaire. 
    
    Compte tenu de ces orbservations, nous pouvons conclure que le modèle n'est pas juste. Afin d'afiner le modèle, on pourrait precéder au nettoyadge des données en éliminant les points abbérrants détectés avec les graphes de résidus.

Pour vérifier si les résidus suivent une loi normale, nous faisons le test de Shapiro-Wilk : 
  
```{r, echo=FALSE}
shapiro.test(residuals(temps.moy_lm))
```
Nous obtenons une p-value supérieure à 5% , on ne peut rien conclure sur la loi des résidus ce qui est cohérent avec l'allure du graphique Normal Q-Q.

## 2.3. Comportement par rapport à la structure du graphe

Nous allons ici utiliser des graphes pre-construits pour étudier l'execution de l'algorithme avec différents graphs.

Nous nous servons d'un dataset contenant des propriétés pour 73 graphes. le temps moyen d’exécution de l’algorithme est déjà résumé par une moyenne pour les executions de l'algorithme.


#### Lecture du fichier 'DonneesTSP.csv'.

````{r, echo=TRUE}
data.graph <- data.frame(read.csv('DonneesTSP.csv'))
````

#### Ajustement du modèle linéaire de $\log(temps.moy)^2$ en fonction de toutes les variables présentes. Modèle sans constante.

Mise en \oe uvre d'une sélection de variables pour ne garder que les variables pertinentes.

````{r, echo=FALSE}
data_temps <- log(data.graph$tps)
data.graph$dim <- sqrt(data.graph$dim)
data.graph$tps <- NULL
data_temps.lm <-lm(data_temps~.,data = data.graph)

summary(data_temps.lm)
````


On créer un modèle linéaire multi-dimensionnel pour l'ensemble des dimensions disponibles sur le dataset contenant les résultats d'exécution  pour les différents graphs. 

On remarque que les coefficients du modèle linéaire obtenu sont plus ou moins importants par rapport à leus valeurs. Les coéfficients les plus faibles correspondent aux paramètres les moins pertinents pour notre étude (par exemple le diamètre). 

#### Mise en \oe uvre d'une sélection de variables pour ne garder que les variables pertinentes.

Nous appliquons la fonction step de R qui nous permet d'éliminer les coefficients non pertienents pour notre étude. 

Nous obtenons le résumé suivant: 

````{r, echo=TRUE}

new_lm <-step(data_temps.lm)
````
Nous remarquons que la fonction step à éliminer la dimension diameter qui n'est donc pas pertinenet pour notre étude afin de minimiser l'AIC. 
Vérifions maintenant la pertinence du nouveau modèle.

````{r, echo=TRUE}
summary(new_lm)
````
Nous remarquons que le coefficient du test de Fisher a augmenté par rapport au modèle avant réduction des degrés de liberté. La p-valeur reste très faible. ainsi le modèle est bien validée. 

#####Remarque par rapport aux paramètres retenus
Compte tenu des résulats obtenu suite à la réduction des degrés de liberté du modèle, nous remarquons que certains coeeficients sont bien retenus bien que ceux ci semblent pas pertienents pour l'étude de manière individuelle. Ceci st le cas pour sd.dist ou encore sd.deg qui ne semblent être d'une grande influence sur le modèle au vu de leur faible coefficient. Cependant, il semble rapporter une plus-value pour l'ensemble du modèle.
#### Analyse de la validité du modèle : 

##### pertinence des coefficients et du modèle,
Nous remarquons des coefficients plus ou moins importants selon leur pertinenence avec des écarts plutôt corrects (en comparaison avec les modèles étudiés auparavant). Les p valeurs sont assez faibles pour la globalité des coefficients. Nous pouvons ainsi conclure de la pertinenece des coefficients du modèle linéaire multidimensionnel. Le vecteur directeur de ce modèle présente des coefficients corrects. La constante à l'origine ne présente pas d'écart important (seulement 0.49 vs 13.10 pour les premiers modèles). 

##### étude des hypothèses sur les résidus.
  
```{r}
par(mfrow=c(2,2)) # 4 graphiques, sur 2 lignes et 2 colonnes
plot(new_lm)
```
    + Les graphiques Residuals vs Fitted et Scale-Location représentent des points peu répartis selon les abscisses ainsi la variance des résidus est constante. Les points présentent une tendance pas très marquée sur le graphique, ainsi l'espérance est nulle.
    + Le graphique Normal Q-Q semble être presque complétement aligné avec un éloignement de la droite au niveau des extrémités, nous aurons donc envie d'affirmer que l'hypothèse de Loi normale pour les résidus est vraie (à confirmer avec d'autres tests cf test de Shapiro-Wilk)
    + Le graphique Residuals vs Leverage montre l'influence des échantillons. Nous ne remarquons pas l'existence d'outliers (points très eloignés des autres ou spécialement en dehors des bornes par rapport à la distance de Cook). Ainsi, une multitude de points contribuent à la construction de la droite du modèle linéaire. 

```{r, echo=FALSE}
shapiro.test(residuals(new_lm))
````
Nous obtenons une p-value = 36.41% > 5% on ne peut conclure que les résidus suivent une loi normale. 