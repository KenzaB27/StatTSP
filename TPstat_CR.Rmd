---
title: "TP Statistique"
author: "BOUZID Kenza, JEANNE Nathan, CANNEDDU Hugo"
date: "1 avril 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
set.seed(287)
knitr::opts_chunk$set(echo = TRUE)

## il est possible qu'avant d'installer le package TSPpackage vous deviez installer ou ré-installer Rcpp
#install.packages('Rcpp')

# install.packages('./TSPpackage_1.0.tar.gz',repos=NULL,type='bin') ## pour linux
# install.packages('./TSPpackage_1.0.zip',repos=NULL,type='bin')    ## pour windows
## je ne peux pas fournir de package pour mac...

## Appels aux packages, après les avoir installés !
library(sp)
library(maps)
library(microbenchmark)
library(TSP)
library(TSPpackage)
```
Voici le plan de ce qui sera fait dans le TP.

# 0. Visualisation de chemins

Lecture du fichier des villes :

```{r, echo=TRUE}
villes <- read.csv('DonneesGPSvilles.csv',header=TRUE,dec='.',sep=';',quote="\"")
str(villes)
```
Représentation des chemins par plus proches voisins et du chemin optimal :
```{r, echo=TRUE}
coord <- cbind(villes$longitude,villes$latitude)
dist <- distanceGPS(coord)
voisins <- TSPnearest(dist)

pathOpt <- c(1,8,9,4,21,13,7,10,3,17,16,20,6,19,15,18,11,5,22,14,12,2)

par(mfrow=c(1,2),mar=c(1,1,2,1))
plotTrace(coord[voisins$chemin,], title='Plus proches voisins')
plotTrace(coord[pathOpt,], title='Chemin optimal')
```


Les longueurs des trajets (à vol d'oiseau) valent respectivement, pour la méthode des plus proches voisins :
```{r, echo=FALSE}
voisins$longueur
```
et pour la méthode optimale :
```{r, echo=FALSE}
calculeLongueur(dist,pathOpt)
```

Ceci illustre bien l'intérêt d'un algorithme de voyageur de commerce. Nous allons dans la suite étudier les performances de cet algorithme.


# 1. Comparaison d'algorithmes

Nombre de sommets fixes et graphes "identiques".

```{r, echo=TRUE}
      n <- 10
sommets <- data.frame(x = runif(n), y = runif(n))
  couts <- distance(sommets)
```

## 1.1. Longueur des chemins

Comparaison des longueurs de différentes méthodes : 

### boxplots
   
```{r, echo=FALSE}
  X1 <- vector(mode="integer", length=50)
  X2 <- vector(mode="integer", length=50)
  X3 <- vector(mode="integer", length=50)
  X4 <- vector(mode="integer", length=50)
  X5 <- vector(mode="integer", length=50)
  n <-10
  for(i in 1:50)
  {
    sommets <- data.frame(x = runif(n), y = runif(n))
    couts <- distance(sommets)
    v1<-TSPbranch(couts)
    v2<-TSPnearest(couts)
    v3<-TSPsolve(couts, "repetitive_nn")
    v4<-TSPsolve(couts, "farthest_insertion")
    v5<-TSPsolve(couts, "two_opt")
    X1[i]<-v1
    X2[i]<-v2$longueur
    X3[i]<-v3
    X4[i]<-v4
    X5[i]<-v5
  }

  
  mat <- cbind(X1,X2,X3,X4,X5)
  colnames(mat)<-c("Branch","Nearest","repetitive_nn","insertion","two_opt")
  par(mfrow=c(1,1))
  boxplot(mat,notch=TRUE)
```   

Les boxplots ci dessus correspondent respectivement aux algorithme "branch", "nearest", "repetitive_nn", "farthest_insertion" et "two_opt". (de gauche à droite).

la boite de l'algorithme Branch & Bound nous permet de voir qu'après 50 essais, cet algorithme obtient la plus petite valeur minimale, maximale et médiane, ce qui en fait l'algorithme avec les meilleurs résultats sur les 50 tests parmi les 5 méthodes de calcul des plus courts chemins disponibles.

On observe sur le diagramme a moustache un léger avantage pour l'algorithme Branch & Bound qui obtient une longueur des chemin hamiltonien moyenne plus courte sur les 50 exécutions.

### test entre 'nearest' et 'branch'

  Nous allons maintenant comparer les algorithmes "nearest" et Branch & Bound. Pour cela, nous allons réaliser un test d'hypothèses paramétriques.

  Soit m<sub>nn</sub> et m<sub>b</sub> les espérances respectives des algorithmes des plus proches voisins et de Branch&Bound. Les hypothèses de notre test vont être :

  * (H<sub>0</sub>) -> m<sub>nn</sub> - m<sub>b</sub> <= 0
  * (H<sub>1</sub>) -> m<sub>nn</sub> - m<sub>b</sub> > 0   
   
Nous cherchons ainsi la p_value afin de rejetter ou non l'hypothèse.
On utilise le code ci-dessous avec X2 les résultats obtenus pour l'algorithme "nearest" et X1 pour Branch & Bound.
Le test porte ici sur l'espérance d'une loi normale (étant donné que les longueurs obtenues suivent une loi normale).

```{r, echo=TRUE}
  t.test(X2, X1, mu=0, paired=TRUE, alternative='greater')
```
On obtient une p_value de l'ordre de 10<sup>-11</sup>. 
Celle-ci est inférieure a alpha = 1%, on rejette donc l'hypothèse (H<sub>0</sub>) -> m<sub>nn</sub> - m<sub>b</sub> <= 0.
On a donc m<sub>nn</sub> > m<sub>b</sub> (H<sub>1</sub>), ou que l'espérance des longueurs obtenues avec Branch & Bound est plus faible que celles obtenues avec "de l'algorithme des plus proches voisins"nearest".

Au vu de données et de nos résultats on peut donc affirmer que l'algorithme Branch & Bound renvoie de meilleurs résultats que l'algorithme "nearest".


### tests 2 à 2 
   
   Nous allons maintenant comparer 2 à 2 les 5 algorithmes à notre dispositions et les longueurs du plus court chemin obtenues au terme des 50 exécutions. On applique ici la procédure de Bonferroni qui consiste à tester les hypothèses suivantes (avec i != j) :
   
  * (H<sub>0</sub>) -> m<sub>i</sub> = m<sub>j</sub>
  * (H<sub>1</sub>) -> m<sub>i</sub> != m<sub>j</sub> 

Avec  m<sub>i</sub> et m<sub>j</sub> la moyenne des longueurs obtenues par 2 algorithmes au choix.
        
Le test nous donne le résultat ci-dessous :
   
```{r, echo=FALSE}
results<- c(X1,X2,X3,X4,X5)
methods<- c(rep("branch", 50),rep("nearest", 50),rep("repetitive_nn", 50),rep("farthest_insertion", 50),rep("two_opt", 50))

pairwise.t.test(results,methods,adjust.method="bonferroni")
```

  On obtient un tableau des différentes p_valeurs obtenues entre les algorithmes 2 à 2.
Pour chaque p-value <= alpha (où alpha représente le risque, par défaut 5%), on rejette H<sub>0</sub>. 
Ainsi pour chaque rejet les algorithmes donnent des longueurs moyennes différentes (H<sub>1</sub>), en revanche ne pas rejeter l'hypothése ne permet pas d'affirmer que les algorithmes ont la même longeur moyenne mais il est possible que leurs résultats soient proches.

Dans nos résultats, les algorithmes dont l'esperance des longueurs obtenues n'est pas identiques (avec un risque alpha = 5%) sont les couples ci-dessous :

  * nearest / branch
  * farthest_insertion / nearest
  * two_opt / branch
  * repetitive_nn / nearest
  * two_opt / repetitive_nn

  On remarque que les algorithmes "nearest" et Branch & Bound ont des espérances des longueurs obtenues différentes, ce qui rejoint les résultats obtenus précédemment.
  

## 1.2. Temps de calcul

Finalement nous allons effectuer une comparaison des temps d'exécution à l'aide du package microbenchmark afin de déterminer l'algorithme le plus performant.

Exemple d'application de microbenchmark :
```{r, echo=TRUE}
m<-microbenchmark(TSPbranch(couts),TSPnearest(couts), TSPsolve(couts, "repetitive_nn"), TSPsolve(couts, "farthest_insertion"), TSPsolve(couts, "two_opt"), times=20, setup={ n<-10
  sommets <- data.frame(x = runif(n), y = runif(n))
  couts <- distance(sommets)
})

summary(m)
```

Avc les résultats obtenues on peut observer que les algorithmes "two_opt" et "nearest" sont les plus rapides et ont donc des temps d'exécution proches (même ordre de grandeur de quelques centaines de secondes). 
On remarque également que l'algorithme Branch & Bound obtient la note **b**, note moyenne ("repetitive_nn" ayant la pire note avec un temps d'exécution moyen 2 fois plus long que le Branch & Bound et 50 fois plus long que "nearest", le plus rapide). 

Branch & Bound est donc moins performand (en temps) que tous les autres algorithmes, cependant, il reste celui proposant les meilleurs résultats. 

# 2. Etude e la complexité de l'algorithme Branch and Bound

## 2.1. Comportement par rapport au nombre de sommets : premier modèle

Récupération du temps sur 10 graphes pour différentes valeurs de $n$.

````{r, echo=TRUE}
seqn <- seq(4,20,1)

#calcul de temps
temps<-matrix(0,nrow=17,ncol=10)
for ( i in 1:length(seqn)){
  temps[i,]<-microbenchmark(TSPsolve(couts, method = "branch"),
                           times = 10,
                           setup = { n <- seqn[i]
                           couts <- distance(cbind(x = runif(n), y = runif(n)))}
  )$time
}
````
Visualisation de temps en fonction de n puis de $\log(temps)^2$ en fonction de n: 

```` {r, echo=FALSE}
# representation de temps

par(mfrow=c(1,2)) # 2 graphiques sur 1 ligne
matplot(seqn, temps, type = 'p', xlab='n', ylab='temps')
matplot(seqn, log(temps)^2, type ='p', xlab='n', ylab=expression(log(temps)^2))
````

Nous remarquons le comportement exponentionnelle du temps en fonction de n, ainsi il s'agit bien d'une régression linéaire pour $\log(temps)^2$  en fonction de n. 

Ajustement du modèle linéaire de $\log(temps)^2$ en fonction de $n$.

```` {r, echo = FALSE}
# Ajuter le modèle linéaire de log(temps)^2

vect_temps <- log(as.vector(temps))^2
vect_dim <- rep(seqn,times=10)
temps.lm <- lm(vect_temps ∼ vect_dim)
summary(temps.lm)

````

#### Analyse de la validité du modèle : 

A partir des résultats obtenus, nous remarquons un ration $R^2$ de `r summary(temps.lm)$r.squared` qui est assez proche de 1. Cela signifie que les observations s'éloignent un peu du modèle prédit. En effet, le graphique observé montre une courbe pas tout à fait linéaire. 

Afin de visualiser l'écart au modèle, nous traçons la courbe suivante: 
```{r,echo=FALSE}
matplot(seqn, log(temps)^2, xlab='n', ylab=expression(log(temps)^2))
abline(temps.lm)
```

Nous remarquons que les données suivent bien un modèle linéaire, cependant l'écart observé reste important. 

#### pertinence des coefficients et du modèle:
  
A partir des informations fournies par R autour du modèle étudié, nous remarquons: 
    + des coefficients avec un taux d'erreur (écart-type) important allant jusqu'à 5.3 pour l'ordonnée à l'origine et 0.4 pour le cofficient directeur, nous ne pouvons donc pas affirmer la pertinence des coefficients du modèle linéaire.
    + la p-valeur de chacun des coefficients est très petite inférieure à $2.2\exp(-16)$

Ainsi, nous rejetons l'hypothèse que le $\log(temps)^2$ d'exécution de l'algorithme Branch and Bound suit un modèle linéaire.
  
#### étude des hypothèses sur les résidus.
  Les hypothèses sur les résidus à étudier sont les suivantes: 
    + Loi normale
    + Espérance nulle 
    + Variance constante 
    + indépendance
  Les graphiques suivants nous permettent de valider oupas ces dernières. 
```{r, echo=FALSE}
par(mfrow=c(2,2)) # 4 graphiques, sur 2 lignes et 2 colonnes
plot(temps.lm)
```

  Compte tenue des résultats obtenus, nous remarquons: 
    + Les graphiques Residuals vs Fitted et Scale-Location représentent des nuages de points très écartés ainsi la variance des résidus n'est pas constante. Ainsi on dit que les résidus sont hétéroscédastiques. Les points ne présentent une tendance trop marquée sur le graphique, nous pouvons ainsi sire que l'espérance est nulle.
    + Le graphique Normal Q-Q semble être presque complétement aligné avec un éloignement de la droite au niveau des extrémités, nous aurons donc envie d'affirmer que l'hypothèse de Loi normale est vraie (à confirmer avec d'autres tests cf test de Shapiro-Wilk)
    + Le graphique Residuals vs Leverage montre l'influence des échantillons. Nous remarquons l'existence d'outliers (points très eloignés des autres). Ainsi, une multitude de points ne contribuent pas à la construction de la droite du modèle linéaire. 

Au vu du rejet de quelques unes des 4 hypothèses, le modèle n'est plus valable. Vérifions si les résidus suivent quand même une loi normale.

Pour vérifier si les résidus suivent une loi normale, nous faisons le test de Shapiro-Wilk : 
  
```{r, echo=FALSE}
shapiro.test(residuals(temps.lm))
```
Nous obtenons une p-valeur très petite <5% ainsi les résidus suivent bien une loi normale. 

## 2.2. Comportement par rapport au nombre de sommets : étude du comportement moyen

Récupération du temps moyen et traçage des courbes: 

```{r}
temps.moy <- rowMeans(temps)
vect_temps_moy <- log(as.vector(temps.moy))^2
vect_dim <- seqn
temps.moy_lm <- lm(vect_temps_moy~vect_dim)
par(mfrow=c(1,2)) # 2 graphiques sur 1 ligne
matplot(vect_dim, temps.moy, xlab='n', ylab='temps')
matplot(vect_dim, vect_temps_moy, xlab='dimension', ylab=expression(log(temps_moy)^2))
```

La courbe *temps_moyen* en fonction de *n* suit une tendance exponentielle. Ainsi comme la 1ère partie, la courbe $\log(temps.moy)^2$ en fonction de $n$ devrait correspondre à un modèle linéaire comme le montre le graphique.

### Ajustement du modèle linéaire de $\log(temps.moy)^2$ en fonction de $n$.

```` {r, echo = FALSE}
# Ajuter le modèle linéaire de log(temps)^2
summary(temps.moy_lm)
````

#### Analyse de la validité du modèle : 

A partir du résumé réalisé par R concernant le modèle linéaire, nous remarquons encore une fois des coefficients pas du tout correctes (car écart très important pour les deux coefficients Intercept et vect_dim allant juqu'à 11,93 pour l'ordonnée à l'origine). De plus les p-valeurs de ces derniers sont très très faibles. Le coeeficient R-squared est égal à 0.936 et se rapproche fortement de 1. 
Nous rejettons ainsi l'hypothèse d'un modèle linéaire.

##### Etude des hypothèses sur les résidus.
  
```{r}
par(mfrow=c(2,2)) # 4 graphiques, sur 2 lignes et 2 colonnes
plot(temps.moy_lm)
```

  + Les graphiques Residuals vs Fitted et Scale-Location représentent des points peu répartis selon les abscisses ainsi la variance des résidus est constante. Les points présentent une tendance trop marquée sur le graphique, ainsi l'espérance n'est pas nulle.
  + Le graphique Normal Q-Q semble n'est pas du tout aligné donc les résidus ne suivent pas une loi normale.
  + Le graphique Residuals vs Leverage montre l'influence des échantillons. Nous ne remarquons pas l'existence d'outliers (points très eloignés des autres ou spécialement en dehors des bornes par rapport à la distance de Cook). Ainsi, une multitude de points contribuent à la construction de la droite du modèle linéaire. 
  
    Compte tenu de ces orbservations, nous pouvons conclure que le modèle n'est pas juste. Afin d'afiner le modèle, on pourrait precéder au nettoyadge des données en éliminant les points abbérrants détectés avec les graphes de résidus.

Pour vérifier si les résidus suivent une loi normale, nous faisons le test de Shapiro-Wilk : 
  
```{r, echo=FALSE}
shapiro.test(residuals(temps.moy_lm))
```
Nous obtenons une p-value supérieure à 5% , on ne peut rien conclure sur la loi des résidus ce qui est cohérent avec l'allure du graphique Normal Q-Q.

## 2.3. Comportement par rapport à la structure du graphe

Nous allons ici utiliser des graphes pre-construits pour étudier l'execution de l'algorithme avec différents graphs.

Nous nous servons d'un dataset contenant des propriétés pour 73 graphes. le temps moyen d’exécution de l’algorithme est déjà résumé par une moyenne pour les executions de l'algorithme.


#### Lecture du fichier 'DonneesTSP.csv'.

````{r, echo=TRUE}
data.graph <- data.frame(read.csv('DonneesTSP.csv'))
````

#### Ajustement du modèle linéaire de $\log(temps.moy)^2$ en fonction de toutes les variables présentes. Modèle sans constante.

Mise en \oe uvre d'une sélection de variables pour ne garder que les variables pertinentes.

````{r, echo=FALSE}
data_temps <- log(data.graph$tps)
data.graph$dim <- sqrt(data.graph$dim)
data.graph$tps <- NULL
data_temps.lm <-lm(data_temps~.,data = data.graph)

summary(data_temps.lm)
````


On créer un modèle linéaire multi-dimensionnel pour l'ensemble des dimensions disponibles sur le dataset contenant les résultats d'exécution  pour les différents graphs. 

On remarque que les coefficients du modèle linéaire obtenu sont plus ou moins importants par rapport à leus valeurs. Les coéfficients les plus faibles correspondent aux paramètres les moins pertinents pour notre étude (par exemple le diamètre). 

#### Mise en \oe uvre d'une sélection de variables pour ne garder que les variables pertinentes.

Nous appliquons la fonction step de R qui nous permet d'éliminer les coefficients non pertienents pour notre étude. 

Nous obtenons le résumé suivant: 

````{r, echo=TRUE}

new_lm <-step(data_temps.lm)
````
Nous remarquons que la fonction step à éliminer la dimension diameter qui n'est donc pas pertinenet pour notre étude afin de minimiser l'AIC. 
Vérifions maintenant la pertinence du nouveau modèle.

````{r, echo=TRUE}
summary(new_lm)
````
Nous remarquons que le coefficient du test de Fisher a augmenté par rapport au modèle avant réduction des degrés de liberté. La p-valeur reste très faible. ainsi le modèle est bien validée. 

##### Remarque par rapport aux paramètres retenus

Compte tenu des résulats obtenu suite à la réduction des degrés de liberté du modèle, nous remarquons que certains coeeficients sont bien retenus bien que ceux ci semblent pas pertienents pour l'étude de manière individuelle. Ceci st le cas pour sd.dist ou encore sd.deg qui ne semblent être d'une grande influence sur le modèle au vu de leur faible coefficient. Cependant, il semble rapporter une plus-value pour l'ensemble du modèle.

#### Analyse de la validité du modèle : 

##### pertinence des coefficients et du modèle,
Nous remarquons des coefficients plus ou moins importants selon leur pertinenence avec des écarts plutôt corrects (en comparaison avec les modèles étudiés auparavant). Les p valeurs sont assez faibles pour la globalité des coefficients. Nous pouvons ainsi conclure de la pertinenece des coefficients du modèle linéaire multidimensionnel. Le vecteur directeur de ce modèle présente des coefficients corrects. La constante à l'origine ne présente pas d'écart important (seulement 0.49 vs 13.10 pour les premiers modèles). 

##### étude des hypothèses sur les résidus.
  
```{r}
par(mfrow=c(2,2)) # 4 graphiques, sur 2 lignes et 2 colonnes
plot(new_lm)
```
    + Les graphiques Residuals vs Fitted et Scale-Location représentent des points peu répartis selon les abscisses ainsi la variance des résidus est constante. Les points présentent une tendance pas très marquée sur le graphique, ainsi l'espérance est nulle.
    + Le graphique Normal Q-Q semble être presque complétement aligné avec un éloignement de la droite au niveau des extrémités, nous aurons donc envie d'affirmer que l'hypothèse de Loi normale pour les résidus est vraie (à confirmer avec d'autres tests cf test de Shapiro-Wilk)
    + Le graphique Residuals vs Leverage montre l'influence des échantillons. Nous ne remarquons pas l'existence d'outliers (points très eloignés des autres ou spécialement en dehors des bornes par rapport à la distance de Cook). Ainsi, une multitude de points contribuent à la construction de la droite du modèle linéaire. 

```{r, echo=FALSE}
shapiro.test(residuals(new_lm))
````
Nous obtenons une p-value = 36.41% > 5% on ne peut conclure que les résidus suivent une loi normale. 